## Module 6 - Business Continuity and Resiliency in Azure 

### Hight Availability and Disaster Recovery

Two essential aspects of resiliency are high availability and disaster recovery:

- High availability (HA) is the ability of the application to continue running in a healthy state despite localized or transient failures. Typically, high availability relies on redundancy of application components and automatic failover.
- Disaster recovery (DR) is the ability to recover from major incidents, such as service disruption that affects an entire region. Disaster recovery provisions include data backup and archiving, and may require manual intervention, such as restoring a database from backup.

---

### Resilliency

Identifying Requirements
Identify the expected recovery time objective and recovery point objective:

- __Recovery time objective (RTO)__ is the maximum acceptable time that an application can be unavailable after an incident. If your RTO is 90 minutes, you must be able to restore the application to a running state within 90 minutes from the start of a disaster. If you have a very low RTO, you might keep a second deployment running in the standby mode.

- __Recovery point objective (RPO)__ is the maximum duration of data loss that is acceptable during a disaster. For example, if you store data in a single database, with no replication to other databases, and perform hourly backups, you could lose up to an hour worth of data.

SLA | Downtime per week | Downtime per month | Downtime per year
--- | --- | --- | ---
99% | 1.68 hours | 7.2 hours |3.65 days
99.9% | 10.1 minutes | 43.2 minutes | 8.76 hours
99.95% | 5 minutes | 21.6 minutes | 4.38 hours
99.99% | 1.01 minutes | 4.32 minutes | 52.56 minutes
99.999% |  6 seconds | 25.9 seconds | 5.26 minutes

Whenever applicable, implement composite SLAs. For example, consider an App Service web app that writes to Azure SQL Database. As of December 2018, these Azure services have the following SLAs:

- __App Service Web Apps__ = 99.95%
- __SQL Database__ = 99.99%

If either service fails, the whole application fails. In general, the probability of each service failing is independent, so the composite SLA for this application is:

>99.95% × 99.99% = 99.94%. 

That's lower than the individual SLAs, which isn't surprising, because an application that relies on multiple services has more potential failure points.

Alternatively, you can improve the composite SLA by creating independent fallback paths. For example, if SQL Database is unavailable, you can store transactions into a queue, to be processed later. As of December 2018, __Azure Service Bus Queues have the 99.9% availability SLA__.

With this design, the application is still available even if it can't connect to the database. However, it fails if the database and the queue both fail at the same time. The expected percentage of time for a simultaneous failure is:

>0.0001 × 0.001

so the composite SLA for this combined path is:

>Database OR queue = 1.0 − (0.0001 × 0.001) = 99.99999%

Effectively, the total composite SLA is:

>Web app AND (database OR queue) = 99.95% × 99.99999% = ~99.95%

__NOTE:__ There are tradeoffs to this approach. The application logic is more complex, you are paying for the queue, and there may be data consistency issues to consider.

Take into account SLAs for multi-region deployments. Another HA technique is to deploy the application in more than one region, and use Azure Traffic Manager to fail over if the application fails in one region.

For a two-region deployment, the composite SLA is calculated as follows:

Let N be the composite SLA for the application deployed in one region. The expected chance that the application will fail in both regions at the same time is:

>(1 − N) × (1 − N).

Combined SLA for both regions:

>= 1 − (1 − N)(1 − N) = N + (1 − N)N

To calculate the composite SLA, you must factor in the SLA for Traffic Manager. As of December 2018, the SLA for Traffic Manager SLA is 99.99%:

>Composite SLA = 99.99% × (combined SLA for both regions)


---

### Application Design

__Failure Mode Analysis (FMA)__

Failure mode | Detection strategy
--- | ---
Service is unavailable | HTTP 5xx
Throttling | HTTP 429 (Too Many Requests)
Authentication | HTTP 401 (Unauthorized)
Slow response | Request times out

__Avoiding a Single Point of Failure for Applications__

- Single VM. Azure provides an uptime SLA for individual VMs, as long as all disks of these VMs are configured to use Premium Storage. Although you can get a higher SLA by running two or more VMs, a single VM may be reliable enough for some workloads. For production workloads, we recommend using two or more VMs for redundancy.

- Availability sets. To protect against localized hardware failures, such as a power unit or a network switch failing, deploy two or more VMs in an availability set. An availability set consists of two or more fault domains, each of which uses a separate power source and network switch. VMs in an availability set are distributed across the fault domains, so localized a hardware failure affects only one fault domain.

- Availability zones. An Availability Zone is a separate physical datacenter within an Azure region. Each Availability Zone has a distinct power source, network, and cooling. Deploying VMs across availability zones helps to protect an application against datacenter-wide failures.

- Azure Site Recovery. Replicate Azure virtual machines to another Azure region for business continuity and disaster recovery needs. You can conduct periodic DR drills to ensure you meet the compliance needs. The VM will be replicated with the specified settings to the selected region so that you can recover your applications in the event of outages in the source region.

- Paired regions. Each Azure region is paired with another region. With the exception of Brazil South, regional pairs are located within the same geography in order to meet data residency requirements for tax and law enforcement jurisdiction purposes.

__Azure Autoscaling and Load Balancing__

- Placing two or more VMs behind a load balancer. The load balancer distributes traffic to all the VMs. If you choose Azure Application Gateway, remember that you need to provision two or more Application Gateway instances to qualify for the availability SLA.

- Scaling out an Azure App Service app to multiple instances. App Service automatically balances load across instances.

- Using Azure Traffic Manager to distribute traffic across a set of endpoints.

__Multi-Region Deployment__

Consider deploying your application across multiple regions. A multi-region deployment can use an active-active pattern (distributing requests across multiple active instances) or an active-passive pattern (keeping a "warm" instance in reserve, in case the primary instance fails).

Use Azure Traffic Manager to route your application's traffic to different regions. Azure Traffic Manager performs load balancing at the DNS level and will route traffic to different regions based on the traffic routing method you specify and the health of your application's endpoints.

Configure and test health probes for your load balancers and traffic managers. Ensure that your health logic checks the critical parts of the system and responds appropriately to health probes. The health probes for Azure Traffic Manager and Azure Load Balancer serve a specific function. For Traffic Manager, the health probe determines whether to fail over to another region. For a load balancer, it determines whether to remove a VM from rotation.

	
. | Availability Set | Availability Zone | Azure Site Recovery/Paired region
--- | --- | --- | ---
Scope of failure | Rack | Datacenter | Region
Request routing |  Load Balancer | Cross-zone Load Balancer | Traffic Manager, Azure Front Door
Network latency | Very low |  Low | Mid to high
Virtual network | VNet | VNet | Cross-region VNet peering

__Enhancing security__

__Ensure application-level protection against distributed denial of service (DDoS)__ attacks. Azure services are protected against DDoS attacks at the network layer. However, Azure cannot protect against application-layer attacks, because it is difficult to distinguish between true user requests from malicious user requests.

__Adhere to the principle of least privilege for access to the application's resources__. The default for access to the application's resources should be as restrictive as possible. Grant higher level permissions on an approval basis. Granting overly permissive access to your application's resources by default can result in someone purposely or accidentally deleting resources. Azure provides role-based access control to manage user privileges, but it's important to verify least privilege permissions for other resources that have their own permissions systems such as SQL Server.


---

### Testing Deployment, and Maintenance

__Deployment and Maintenance Tasks__

__Automate and test deployment and maintenance tasks.__ Distributed applications consist of multiple parts that must work together. The deployment process should be predictable and repeatable. In Azure, this process might include provisioning Azure resources, deploying application code, and applying configuration settings:

- Use Azure Resource Manager templates to automate provisioning of Azure resources.
- Use Azure Automation Desired State Configuration (DSC) to configure VMs.
- Use an automated deployment process for application code.

__For App Service deployments, store configuration as app settings.__ Define the settings in your Resource Manager templates, or by using PowerShell, so that you can apply them as part of an automated deployment / update process, which is more reliable.

__Give resources meaningful names.__ Giving resources meaningful names makes it easier to locate a specific resource and understand its role.

__Organize resource groups by function and lifecycle.__ In general, a resource group should contain resources that share the same lifecycle. This makes it easier to manage deployments, delete test deployments, and assign access rights, reducing the chance that a production deployment is accidentally deleted or modified. Create separate resource groups for production, development, and test environments. In a multi-region deployment, put resources for each region into separate resource groups. This makes it easier to redeploy one region without affecting the other region(s).

__Infrastructure as Code and Immutable Infrastructure__

Below are summaries forthe principles of infrastructure as code and immutable infrastructure:

- __Infrastructure as code is the practice of using code to provision and configure infrastructure.__ Infrastructure as code may use a declarative approach or an imperative approach (or a combination of both). Resource Manager templates constitute an example of a declarative approach. PowerShell scripts constitute an example of an imperative approach.

- __Immutable infrastructure is the principle that you shouldn’t modify infrastructure after it’s deployed to production.__ Otherwise, you can get into a state where ad hoc changes have been applied, so it's hard to know exactly what changed, and hard to reason about the system.

__Maximize Application Availability__

Design your release process to maximize application availability. If your release process requires services to go offline during deployment, your application will be unavailable until they come back online. Use the blue/green or canary release deployment technique to deploy your application to production. Both of these techniques involve deploying your release code alongside production code so users of release code can be redirected to production code in the event of a failure:

- __Blue-green deployment__ is a technique where an update is deployed into a production environment separate from the live application. After you validate the deployment, switch the traffic routing to the updated version. For example, Azure App Service Web Apps enables this with staging slots.

- __Canary releases__ are similar to blue-green deployments. Instead of switching all traffic to the updated version, you roll out the update to a small percentage of users, by routing a portion of the traffic to the new deployment. If there is a problem, back off and revert to the old deployment. Otherwise, route more of the traffic to the new version, until it gets 100% of the traffic.

__Additional Considerations for Testing, Deployment, and Maintenance__

- __Have a rollback plan for deployment.__ It's possible that your application deployment could fail and cause your application to become unavailable. Design a rollback process to go back to a last known good version and minimize downtime.

- __Ensure that your application does not run up against Azure subscription limits.__ Azure subscriptions have limits on certain resource types, such as number of resource groups, number of cores, and number of storage accounts. If your application requirements exceed Azure subscription limits, create another Azure subscription and provision sufficient resources there.

- __Ensure that your application does not run up against per-service limits.__ Individual Azure services have consumption limits — for example, limits on storage, throughput, number of connections, requests per second, and other metrics. Your application will fail if it attempts to use resources beyond these limits. This will result in service throttling and possible downtime for affected users. Depending on the specific service and your application requirements, you can often avoid these limits by scaling up (for example, choosing another pricing tier) or scaling out (adding new instances).

- __Perform fault injection testing of your applications.__ Test the resiliency of the system during failures, either by triggering actual failures or by simulating them. Your application can fail for many different reasons, such as certificate expiration, exhaustion of system resources in a VM, or storage failures. Test your application in an environment as close as possible to production, by simulating or triggering real failures. For example, delete certificates, artificially consume system resources, or delete a storage source. Verify your application's ability to recover from all types of faults, alone and in combination. Check that failures are not propagating or cascading through your system.

- __Perform load testing of your applications.__ Load testing is crucial for identifying failures that only happen under load, such as the backend database being overwhelmed or service throttling. Test for peak load, using production data or synthetic data that is as close to production data as possible. The goal is to see how the application behaves under real-world conditions.

- __Run tests in production using both synthetic and real user data__. Test and production are rarely identical, so it's important to use blue/green or a canary deployment and test your application in production. This allows you to test your application in production under real load and ensure it will function as expected when fully deployed.

- __Establish a process for interacting with Azure support.__ If the process for contacting Azure support is not set before the need to contact support arises, downtime will be prolonged as the support process is navigated for the first time. Include the process for contacting support and escalating issues as part of your application's resiliency from the outset.

- __Use resource locks for critical resources, such as VMs.__ Resource locks prevent an operator from accidentally deleting a resource.

---

### Data Management

Replicating data is a general strategy for handling non-transient failures in a data store. Many storage technologies provide built-in replication. It's important to consider both the read and write paths. Depending on the storage technology, you might have multiple writable replicas, or a single writable replica and multiple read-only replicas. To maximize availability, replicas can be placed in multiple regions. However, this increases the latency when replicating the data. Typically, replicating across regions is done asynchronously, which implies an eventual consistency model and potential data loss if a replica fails.

- __Geo-replicate databases.__ Azure SQL Database and Azure Cosmos DB both support geo-replication, which enables you to configure secondary database replicas in other regions. Secondary databases are available for querying and for failover in the case of a data center outage or the inability to connect to the primary database. With Azure SQL Database, you can create auto-failover groups, which facilitate automatic failover. Azure Cosmos DB additionally supports multi-master configuration, with multiple write regions and customizable conflict resolution mechanism.

- __Geo-replicate data in Azure Storage.__ Data in Azure Storage is automatically replicated within a datacenter. For higher availability, use Read-access geo-redundant storage (RA-GRS), which replicates your data to a secondary region and provides read-only access to the data in that region. The data is durable even in the case of a complete regional outage or a disaster.

- __For VMs, do not rely on RA-GRS replication to restore the VM disks (VHD files).__ Instead, use Azure Backup. In addition, consider using managed disks. Managed disks provide enhanced resiliency for VMs in an availability set, because the disks are sufficiently isolated from each other to avoid single points of failure. In addition, managed disks eliminate the need to account for the storage account-level IOPS limits.

__Additional Data Management Considerations__

Below are additional considerations for managing data.

- Sharding. Consider using sharding to partition a database horizontally. Sharding can provide fault isolation and eliminate constraints imposed by database size limits.
- Optimistic concurrency and eventual consistency. Transactions that block access to resources through locking (pessimistic concurrency) can cause poor performance and considerably reduce availability. These problems can become especially acute in distributed systems. In many cases, careful design and techniques such as partitioning can minimize the chances of conflicting updates. Where data is replicated, or is read from a separately updated store, the data will only be eventually consistent. But the advantages usually far outweigh the impact on availability of using transactions to ensure immediate consistency.
- Document data source fail over and fail back processes, and then test it. In the case where your data source fails catastrophically, a human operator will have to follow a set of documented instructions to fail over to a new data source. Regularly test the instruction steps to verify that an operator following them is able to successfully fail over and fail back the data source.
- Periodic backup and point-in-time restore. Regularly and automatically back up data and verify you can reliably restore both the data and the application. Ensure that backups meet your Recovery Point Objective (RPO). The backup process must be secure to protect the data in transit and at rest.
- Ensure that no single user account has access to both production and backup data. Your data backups are compromised if one single user account has permission to write to both production and backup sources. A malicious user could purposely delete all your data, while a regular user could accidentally delete it. Design your application to limit the permissions of each user account so that only the users that require write access have write access and it's only to either production or backup, but not both.
- Validate your data backups. Regularly verify that your backup data is what you expect by running a script to validate data integrity, schema, and queries. There's no point having a backup if it's not useful to restore your data sources. Log and report any inconsistencies so the backup service can be repaired.




---
### Monitoring and Disaster Recovery



---
